# -*- coding: utf-8 -*-
"""PytorchTemplate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uuol2VXW231br0njj_0b-v1Np5KOLg9h
"""

from os.path import exists
from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag
platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())
cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\.\([0-9]*\)\.\([0-9]*\)$/cu\1\2/'
accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'

from google.colab import widgets
import os
import argparse
from torch import LongTensor, FloatTensor
import torch
import torch.nn as nn
from torch.autograd import Variable
import torch.autograd as autograd
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import pandas as pd
from torch.nn.utils.rnn import PackedSequence, pad_packed_sequence, pack_padded_sequence, pack_sequence, pad_sequence

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer
from nltk.stem import LancasterStemmer, PorterStemmer,WordNetLemmatizer

import re
import string
import unicodedata 
import os
import codecs
import random
import logging
import json
import itertools
import math
import copy
import numpy as np
import time
import sys
import gc
import matplotlib.pyplot as plt
from torch.utils.data.dataset import Dataset
from collections import Counter, defaultdict
from easydict import EasyDict as edict

# %matplotlib inline

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

path_to_folder = '/content/gdrive/My Drive/'

!mkdir -p checkpoints

# CUSTOM DATASET

class CustomDatasetFromImages(Dataset):
    def __init__(self, csv_path):
        """
        Args:
            csv_path (string): path to csv file
            img_path (string): path to the folder where images are
            transform: pytorch transforms for transforms and tensor conversion
        """
        self.to_tensor = transforms.ToTensor()
        self.data_info = pd.read_csv(csv_path, header=None)
        # First column contains the image paths
        self.image_arr = np.asarray(self.data_info.iloc[:, 0])
        # Second column is the labels
        self.label_arr = np.asarray(self.data_info.iloc[:, 1])
        # Third column is for an operation indicator
        self.operation_arr = np.asarray(self.data_info.iloc[:, 2])
        self.data_len = len(self.data_info.index)

    def __getitem__(self, index):
        # Get image name from the pandas df
        single_image_name = self.image_arr[index]
        img_as_img = Image.open(single_image_name)

        some_operation = self.operation_arr[index]
        if some_operation:
            pass
        img_as_tensor = self.to_tensor(img_as_img)
        single_image_label = self.label_arr[index]

        return (img_as_tensor, single_image_label)

    def __len__(self):
        return self.data_len


class CustomDatasetFromCSV(Dataset):
    def __init__(self, csv_path, height, width, transform=None):
        self.data = pd.read_csv(csv_path)
        self.labels = np.asarray(self.data.iloc[:, 0])
        self.height = height
        self.width = width
        self.transform = transform

    def __getitem__(self, index):
        single_image_label = self.labels[index]
        # Read each 784 pixels and reshape the 1D array ([784]) to 2D array ([28,28])
        img_as_np = np.asarray(self.data.iloc[index][1:]).reshape(28, 28).astype('uint8')
        # Convert image from numpy array to PIL image, mode 'L' is for grayscale
        img_as_img = Image.fromarray(img_as_np)
        img_as_img = img_as_img.convert('L')
        if self.transform is not None:
            img_as_tensor = self.transform(img_as_img)
        return (img_as_tensor, single_image_label)

    def __len__(self):
        return len(self.data.index)

def get_datasets():
    train_dataset = datasets.MNIST(root='./data/mnist',
                                   train=True,
                                   transform=transforms.ToTensor(),
                                   download=True)
    test_dataset = datasets.MNIST(root='./data/mnist',
                                  train=False,
                                  transform=transforms.ToTensor())
    return train_dataset, test_dataset
  
def get_dataloaders(train_dataset, test_dataset, args, shuffle, kwargs):
    train_loader = torch.utils.data.DataLoader(
            dataset=train_dataset,
            batch_size=args.train_batch_size,
            num_workers=0,
            shuffle=shuffle)
            #**kwargs)
    test_loader = torch.utils.data.DataLoader(
            dataset=test_dataset,
            batch_size=args.test_batch_size,
            num_workers=0,
            shuffle=not shuffle)
            #**kwargs)
    
    return train_loader, test_loader

def data_preview(dataset):
    plt.figure(figsize=(10,10))

    sample = dataset.train_data[:64]
    # shape (64, 28, 28)
    sample = sample.reshape(8,8,28,28)
    # shape (8, 8, 28, 28)
    sample = sample.permute(0,2,1,3)
    # shape (8, 28, 8, 28)
    sample = sample.reshape(8*28,8*28)
    # shape (8*28, 8*28)
    plt.imshow(sample)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.title('First 64 MNIST digits in training set')
    plt.show()

    # print('Labels:', dataset.train_labels[:64].numpy())
    
# EXPONENTIALLY DECAYED MOVING AVERAGE 
    
class AverageBase(object):
    
    def __init__(self, value=0):
        self.value = float(value) if value is not None else None
       
    def __str__(self):
        return str(round(self.value, 4))
    
    def __repr__(self):
        return self.value
    
    def __format__(self, fmt):
        return self.value.__format__(fmt)
    
    def __float__(self):
        return self.value
    

class RunningAverage(AverageBase):
    """
    Keeps track of a cumulative moving average (CMA).
    """
    
    def __init__(self, value=0, count=0):
        super(RunningAverage, self).__init__(value)
        self.count = count
        
    def update(self, value):
        self.value = (self.value * self.count + float(value))
        self.count += 1
        self.value /= self.count
        return self.value


class MovingAverage(AverageBase):
    """
    An exponentially decaying moving average (EMA).
    """
    
    def __init__(self, alpha=0.99):
        super(MovingAverage, self).__init__(None)
        self.alpha = alpha
        
    def update(self, value):
        if self.value is None:
            self.value = float(value)
        else:
            self.value = self.alpha * self.value + (1 - self.alpha) * float(value)
        return self.value
      
      
# https://github.com/tqdm/tqdm 

from IPython.display import HTML, display
class ProgressMonitor(object):
    # Custom IPython progress bar for training
    
    tmpl = """
        <p>Loss: {loss:0.4f}   {value} / {length}</p>
        <progress value='{value}' max='{length}', style='width: 100%'>{value}</progress>
    """

    def __init__(self, length):
        self.length = length
        self.count = 0
        self.move_cnt = 0
        self.display = display(self.html(0, 0), display_id=True)
        
    def html(self, count, loss):
        return HTML(self.tmpl.format(length=self.length, value=count, loss=loss))
        
    def update(self, count, loss, frequency):
        self.count += count
        self.move_cnt += 1
        if not self.move_cnt % frequency:
            self.display.update(self.html(self.count, loss))

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.layer1 = nn.Sequential(
          nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=2),   
          #nn.BatchNorm2d(16),
          nn.ReLU(),
          nn.MaxPool2d(kernel_size=2, stride=2)
        )
        
        self.layer2 = nn.Sequential(
          nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=2),   
          #nn.BatchNorm2d(16),
          nn.ReLU(),
          nn.MaxPool2d(kernel_size=2, stride=2)
        )
        
        self.fully = nn.Linear(8*8*32, 10)
        self.softmax = nn.Softmax()
        #self.softmax = nn.Softmax
        
        
    def forward(self, x):
        t_x = self.layer1(x)
        t_x = self.layer2(t_x)
        t_x = t_x.reshape(t_x.size(0), -1)
        t_x = self.fully(t_x)
        return self.softmax(t_x)

# UTILS

def save_checkpoint(optimizer, model, epoch, filename):
    checkpoint_dict = {
        'optimizer': optimizer.state_dict(),
        'model': model.state_dict(),
        'epoch': epoch
    }
    torch.save(checkpoint_dict, filename)


def load_checkpoint(optimizer, model, filename):
    checkpoint_dict = torch.load(filename)
    epoch = checkpoint_dict['epoch']
    model.load_state_dict(checkpoint_dict['model'])
    if optimizer is not None:
        optimizer.load_state_dict(checkpoint_dict['optimizer'])
    return epoch

def draw_losses(train_losses, test_losses):
    epochs = range(1, len(train_losses) + 1)

    plt.figure(figsize=(10,6))
    plt.plot(epochs, train_losses, '-o', label='Training loss')
    plt.plot(epochs, test_losses, '-o', label='Validation loss')
    plt.legend()
    plt.title('Learning curves')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.xticks(epochs)
    plt.show()

# TRAINING AND TESTING

def train(args, model, device, train_loader, optimizer, epoch, grid, train_size):
    with grid.output_to(0,0):
            progress = ProgressMonitor(length=train_size)
  
    progress = ProgressMonitor(length=train_size)
    train_loss = MovingAverage()
    criterion = nn.NLLLoss()
    model.train()
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        train_loss.update(loss)
        with grid.output_to(0,0):
            progress.update(data.shape[0], train_loss, args.frequency)
        
        if batch_idx % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader)))
    
    return train_loss.value
    
def test(args, model, device, test_loader, grid):
    model.eval()
    criterion = nn.NLLLoss()
    test_loss = RunningAverage()
    correct = 0
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(test_loader):
            data, target = data.to(device), target.to(device)
            output = model(data)
            
            loss = criterion(output, target)
            test_loss.update(loss)
            # F.nll_loss(output, target, reduction='sum').item() # sum up batch loss
            
            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()
    
    print('\nTest set: Accuracy: {}/{} ({:.0f}%)\n'.format(
        correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))
    
    return test_loss.value 
  
def main_loop(args, device, train_loader, test_loader, train_size):
    model = Net().to(device)
    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)
    
    #epoch = load_checkpoint(optimizer, model, 'checkpoints/mnist-010.pkl')
    #print('Resuming training from epoch', epoch)
    grid = widgets.Grid(2,1)
    
    train_losses = []
    test_losses = []
    
    for epoch in range(1, args.epochs + 1):
        train_loss = train(args, model, device, train_loader, optimizer, epoch, grid, train_size)
        train_losses.append(train_loss)
        
        test_loss = test(args, model, device, test_loader, grid)
        test_losses.append(test_loss)

        #checkpoint_filename = 'checkpoints/mnist-{:03d}.pkl'.format(epoch)
        #save_checkpoint(optimizer, model, epoch, checkpoint_filename)
        # Plot loss
        with grid.output_to(1, 0):
            grid.clear_cell()
    
    if args.save_model:
        torch.save(model.state_dict(),"mnist_cnn.pt")
    
    draw_losses(train_losses, test_losses)

if __name__ == '__main__':
    '''
    parser = argparse.ArgumentParser(description='Pytorch MNIST')
    parser.add_argument('--batch-size', type=int, default=64, metavar='N',
                       help='input batch size for training (default: 64)')    
    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
                        help='input batch size for testing (default: 1000)')
    parser.add_argument('--epochs', type=int, default=10, metavar='N',
                        help='number of epochs to train (default: 10)')
    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',
                        help='learning rate (default: 0.01)')
    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',
                        help='SGD momentum (default: 0.5)')
    parser.add_argument('--no-cuda', action='store_true', default=False,
                        help='disables CUDA training')
    parser.add_argument('--seed', type=int, default=1, metavar='S',
                        help='random seed (default: 1)')
    parser.add_argument('--log-interval', type=int, default=10, metavar='N',
                        help='how many batches to wait before logging training status')
    parser.add_argument('--save-model', action='store_true', default=False,
                        help='For Saving the current Model') 
    args = parser.parse_args()
    '''
    
    args = edict({
        'train_batch_size': 64,
        'test_batch_size': 1000,
        'epochs': 10,
        'lr': 0.01,
        'momentum': 0.5,
        'no_cuda': False,
        'seed': 1,
        'log_interval': 50,
        'save_model': False,
        'frequency': 1
    })
    
    use_cuda = not args.no_cuda and torch.cuda.is_available()
    
    torch.manual_seed(args.seed)
    device = torch.device("cuda" if use_cuda else "cpu")
    is_in_datasets = True
    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}
    
    if is_in_datasets:
        train_dataset, test_dataset = get_datasets()
        # data_preview(train_dataset)
        train_loader, test_loader = get_dataloaders(
            train_dataset, test_dataset, args, True, kwargs)
        main_loop(args, device, train_loader, test_loader, len(train_dataset))
    else:
        train_data = pd.read_csv(os.path.join(path_to_folder, 'mnist_train.csv'))
        train_tensor = torch.tensor(train_data.values)
        test_data = pd.read_csv(os.path.join(path_to_folder, 'mnist_test.csv'))
        test_tensor = torch.tensor(test_data.values)
        
        main_loop(args, device, train_tensor, test_tensor, len(train_data))